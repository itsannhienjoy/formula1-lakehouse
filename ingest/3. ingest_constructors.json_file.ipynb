{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c25b39d-c7f4-4d66-8cb8-f05ffd47f88c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read in json file using spark df reader\n",
    "constructors_schema = \"constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e637de6-972a-41df-b80e-ebb971095c34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read data in dataframe\n",
    "constructor_df = spark.read \\\n",
    ".schema(constructors_schema) \\\n",
    ".json(\"/mnt/jdoformula1dl/raw/constructors.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01c490db-b26d-4bdf-a648-bd750b67c776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop irrelevant columns from the df\n",
    "from pyspark.sql.functions import col\n",
    "constructor_df = constructor_df.drop(col('url'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7ac025c-9712-4096-851a-6b219b4e1e09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rename columns and add ingestion date column\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "constructor_final_df = constructor_df.withColumnRenamed(\"constructorId\", \"constructor_id\") \\\n",
    ".withColumnRenamed(\"constructorRef\", \"constructor_ref\") \\\n",
    ".withColumn(\"ingestion_date\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "277df2d2-088f-40f6-a83f-075bb08752e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write data to datalake as parquet\n",
    "constructor_final_df.write.mode(\"overwrite\").parquet(\"/mnt/jdoformula1dl/processed/constructors\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3. ingest_constructors.json_file",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
